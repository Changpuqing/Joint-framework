# Multi-Agent Reinforcement Learning System

## Table of Contents

- [Project Overview](#project-overview)
  - [Background](#background)
  - [Application Scenarios](#application-scenarios)
  - [Key Features](#key-features)
- [Project Structure](#project-structure)
- [Environment Requirements](#environment-requirements)
- [Installation Guide](#installation-guide)
- [Usage Instructions](#usage-instructions)
  - [Training New Models](#training-new-models)
  - [Modifying Training Parameters](#modifying-training-parameters)
  - [Environment Parameter Adjustments](#environment-parameter-adjustments)
  - [Visualizing Results](#visualizing-results)
- [Code Relationships](#code-relationships)
  - [Code Flow Diagram](#code-flow-diagram)
  - [File Dependencies](#file-dependencies)
  - [Algorithm Execution Flow](#algorithm-execution-flow)
- [Algorithm Descriptions](#algorithm-descriptions)
  - [MASAC](#masac-multi-agent-soft-actor-critic)
  - [MATD3](#matd3-multi-agent-twin-delayed-deep-deterministic-policy-gradient)
- [Environment Setup](#environment-setup)
- [Experimental Results](#experimental-results)
  - [Result Visualization](#result-visualization)
  - [Key Performance Metrics](#key-performance-metrics)
  - [Algorithm Comparison](#algorithm-comparison)
- [Contribution Guidelines](#contribution-guidelines)
- [License](#license)

## Project Overview

This project is based on the research work "Joint Node Selection and Task Offloading via Evolutionary Game and MATD3 in UAV-Assisted MEC Networks". It implements and extends multi-agent reinforcement learning algorithms for solving complex decision-making problems in UAV-assisted mobile edge computing (MEC) scenarios. The system models a three-layer air-ground integrated network comprising ground edge servers, UAVs, and users, and proposes a two-stage optimization framework for node selection and task offloading.

### Background

With the rapid proliferation of IoT applications, computation-intensive tasks generated by end devices have grown exponentially, driving the need for low-latency, high-bandwidth, and high-computing-power networks. Traditional cloud computing suffers from high latency and energy consumption, while Mobile Edge Computing (MEC) brings computing resources closer to users, reducing latency and energy use. UAV-assisted MEC systems further enhance flexibility and coverage, but face challenges such as local congestion, resource imbalance, and dynamic user access. This project addresses these challenges by integrating evolutionary game theory for node selection and deep multi-agent reinforcement learning (MATD3) for joint task offloading and power allocation.

### Key Features (updated)

- **System Modeling**: Implements a comprehensive three-layer air-ground integrated network with ground edge servers, UAVs, and distributed users. Models include detailed system, communication, and computation models under partial task offloading.
- **Two-Stage Optimization**: Integrates evolutionary game theory for dynamic node selection and deep reinforcement learning (MATD3) for joint offloading and power control, formulated as a Markov decision process.
- **Algorithm Innovation**: Introduces an evolutionary game-based node selection algorithm for load balancing and a MATD3-based joint control algorithm for offloading ratio and transmission power.
- **Experimental Validation**: Demonstrates significant improvements in delay and energy consumption (reducing total delay by ~33.4% and energy by ~29.4%) compared to existing strategies, with strong scalability and generalizability.
- **Rich Application Scenarios**: Wireless resource allocation, edge computing task scheduling, multi-robot collaboration, and intelligent transportation systems.

### MATD3 (Multi-Agent Twin Delayed Deep Deterministic Policy Gradient)

MATD3 is a multi-agent extension of the TD3 algorithm, designed for UAV-assisted MEC systems to address the joint optimization of node selection, task offloading, and power allocation. The framework consists of:

1. **System Modeling**: A dynamic three-layer air-ground network with macro base stations, multiple UAVs (each with edge servers), and user devices. Users dynamically select nodes and offload tasks based on network load and resource status.
2. **Two-Stage Optimization**:
   - **Node Selection**: An evolutionary game mechanism enables users to autonomously adjust offloading paths, achieving load balancing across nodes.
   - **Task Offloading & Power Control**: The joint control problem is formulated as a Markov decision process, solved using MATD3 for optimal offloading ratios and transmission power.
3. **Algorithm Implementation**: MATD3 leverages a dual-critic architecture and policy smoothing for stable learning and precise policy optimization in continuous action spaces.
4. **Experimental Results**: The framework outperforms traditional strategies in delay control, energy management, and resource scheduling, validated across multiple simulation environments.

**Main Contributions**:
- Proposes a two-stage decision-making framework combining evolutionary game theory and deep reinforcement learning for UAV-assisted MEC.
- Achieves effective dynamic load balancing and optimal resource allocation under practical constraints.
- Demonstrates superior performance in terms of delay, energy efficiency, and scalability.

### Shared Components Between Algorithms

#### MASAC Algorithm Execution Flow

1. **Initialization Phase**:
   - [`MASAC_main_07.py`](./MASAC/MASAC_main_07.py) creates the environment [`env_90.py`](./MASAC/env_90.py)
   - Initializes the experience replay buffer [`replay_buffer.py`](./MASAC/replay_buffer.py)
   - Creates MASAC agents [`MASAC_5072.py`](./MASAC/MASAC_5072.py), which use network structures defined in [`MASAC_network_5072.py`](./MASAC/MASAC_network_5072.py)
   - Initializes the state normalizer [`normalization.py`](./MASAC/normalization.py)

2. **Training Phase**:
   - Environment generates initial state
   - Agents select actions based on state
   - Environment executes actions and returns rewards, next states
   - Experiences are stored in the replay buffer
   - Batch data is sampled from the replay buffer for training
   - Actor and Critic networks are updated
   - Temperature parameter alpha is adaptively adjusted

3. **Evaluation and Visualization**:
   - Agent performance is periodically evaluated
   - [`draw_agri_compa.py`](./MASAC/draw_agri_compa.py) is used to visualize training results

#### MATD3 Algorithm Execution Flow

1. **Initialization Phase**:
   - [`MATD3_main_519_1.py`](./MATD3_final/MATD3_main_519_1.py) creates the environment [`env_519.py`](./MATD3_final/env_519.py)
   - Initializes the experience replay buffer [`replay_buffer.py`](./MATD3_final/replay_buffer.py)
   - Creates MATD3 agents [`matd3_514.py`](./MATD3_final/matd3_514.py), which use network structures defined in [`networks.py`](./MATD3_final/networks.py)
   - Initializes the state normalizer [`normalization.py`](./MATD3_final/normalization.py)

2. **Training Phase**:
   - Environment generates initial state
   - Agents select actions based on state
   - Environment executes actions and returns rewards, next states
   - Experiences are stored in the replay buffer
   - Batch data is sampled from the replay buffer for training
   - Critic networks are updated
   - Actor networks are updated with delay (once every two Critic updates)
   - Target networks are soft-updated

3. **Evaluation and Visualization**:
   - Agent performance is periodically evaluated
   - [`draw.py`](./MATD3_final/draw.py) is used to visualize training results

## Algorithm Descriptions

### MASAC (Multi-Agent Soft Actor-Critic)

MASAC is a multi-agent algorithm based on maximum entropy reinforcement learning, combining the advantages of policy gradients and Q-learning. The algorithm optimizes policies by maximizing the weighted sum of cumulative rewards and policy entropy, which helps exploration and avoids premature convergence to suboptimal policies.

**Key Features**:
- Uses stochastic policies instead of deterministic policies
- Includes entropy regularization term to encourage exploration
- Uses soft state value functions and soft Q functions
- Supports automatic adjustment of temperature parameter alpha

### MATD3 (Multi-Agent Twin Delayed Deep Deterministic Policy Gradient)

MATD3 is a multi-agent extension of the TD3 algorithm, which addresses the overestimation problem in DDPG algorithms through the following mechanisms:
1. Using twin Q networks: Maintains two Q networks, taking the smaller value to mitigate overestimation
2. Delayed policy updates: Reduces policy update frequency, improving stability
3. Target policy smoothing: Adds noise to target actions, reducing policy overfitting

**Key Features**:
- Uses deterministic policies
- Adopts Actor-Critic architecture
- Mitigates overestimation problem through twin Q networks
- Improves training stability through delayed policy network updates

**Application Case**:

To address the challenges of local congestion and imbalanced resource allocation in UAV-assisted air-ground integrated networks, this study constructs a three-layer network architecture comprising ground edge servers, UAVs, and users. A two-stage optimization framework is proposed, incorporating the node selection and task offloading strategies. First, to mitigate the computational resource limitations arising from the dynamic distribution of multiple users, an evolutionary game-based node selection algorithm is developed to achieve effective dynamic load balancing across offloading nodes. Subsequently, the joint task offloading and power allocation problem is formulated as a Markov decision process, and a reinforcement learning algorithm based on MATD3 is designed to attain the joint optimal control of user offloading ratios and transmission power levels. Simulation results demonstrate that the proposed framework reduces total delay and energy consumption by approximately 33.4% and 29.4%, respectively, outperforming the existing strategies. The framework demonstrates superior scalability and energy efficiency in task-intensive scenarios.

## Environment Setup

The environments in the project simulate multi-agent interactions in specific scenarios, including:
- Multiple agents (default is 5)
- Each agent has its own observation space and action space
- Agents need to cooperate or compete to optimize overall performance

**Environment Parameters**:
- Observation dimensions: 3 dimensions per agent
- Action dimensions: 3 dimensions per agent
- Reward calculation: Considers factors such as delay, energy consumption, etc.

## Experimental Results

The project contains experimental results of various algorithms under different environment parameters, mainly stored in subdirectories under each algorithm directory.

### Result Visualization

- MASAC algorithm results can be visualized through [`MASAC/draw_agri_compa.py`](./MASAC/draw_agri_compa.py)
- MATD3 algorithm results can be visualized through [`MATD3_final/draw.py`](./MATD3_final/draw.py)
- Comparison results of different algorithms can be visualized through [`draw_final/draw.py`](./draw_final/draw.py) and [`draw_final/draw_diff_buffer.py`](./draw_final/draw_diff_buffer.py)

### Key Performance Metrics

1. **Reward Curves**: Show the average reward changes during the training process
2. **Delay Metrics**: Show task processing delays in the system
3. **Energy Consumption Metrics**: Show energy consumption in the system
4. **Throughput**: Show the efficiency of task processing in the system

### Algorithm Comparison

Performance comparison of various algorithms (MASAC, MATD3, SAC, TD3) implemented in the project under the same environment:

1. **Convergence Speed**: MASAC and MATD3 typically converge faster than single-agent algorithms
2. **Stability**: MATD3, due to its use of twin Q networks and delayed updates, is typically more stable than MASAC
3. **Final Performance**: Multi-agent algorithms typically outperform single-agent algorithms in cooperative tasks

Experimental results are saved in subdirectories under each algorithm directory, including:
- Reward curves
- Energy consumption
- Delay data
- TensorBoard event files

You can visualize these results using the provided plotting tools.

## Contribution Guidelines

Contributions to the project are welcome! Please follow these steps:

1. Fork this repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Code Style

- Please follow the PEP 8 Python code style guide
- Add appropriate docstrings for all functions and classes
- Use meaningful variable and function names
- Add necessary comments, especially for complex algorithm parts

### Adding New Algorithms

If you want to add new reinforcement learning algorithms, please follow this structure:

1. Create a new algorithm directory in the project root
2. Implement the core classes and network structures of the algorithm
3. Create a main program file for training and testing
4. Add environment files, ensuring compatibility with existing environments
5. Add visualization tools for displaying results
6. Update README.md, adding descriptions of the new algorithm

## License

This project is licensed under the MIT License - see the LICENSE file for details
